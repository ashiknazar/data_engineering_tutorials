{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/arch1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow One-Node Architecture\n",
    "\n",
    "Apache Airflow is an open-source platform for authoring, scheduling, and monitoring workflows. In a one-node architecture, all core components of Airflow run on a single machine. This setup is often used for development, testing, or small-scale deployments.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "In a one-node setup, the following key components run on the same machine:\n",
    "\n",
    "1. **Scheduler**: Manages the execution of tasks by scheduling and monitoring dependencies.\n",
    "2. **Web Server (UI)**: Provides a web-based interface for managing and visualizing workflows.\n",
    "3. **Metadata Database**: Stores DAGs (Directed Acyclic Graphs), task states, logs, and other metadata.\n",
    "4. **Executor**: Executes the tasks. Common executors for one-node setups include `SequentialExecutor` and `LocalExecutor`.\n",
    "5. **Worker** (optional): Handles task execution if using `CeleryExecutor`, though often not used in single-node setups.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "| Component          | Description                                                                                   |\n",
    "|--------------------|-----------------------------------------------------------------------------------------------|\n",
    "| **Scheduler**      | Orchestrates the execution of tasks by scheduling and monitoring dependencies.                |\n",
    "| **Web Server (UI)**| Flask-based web interface for monitoring and managing DAGs and task instances.                |\n",
    "| **Metadata DB**    | Stores the state of DAGs, tasks, and logs. Uses SQLite by default in single-node setups.      |\n",
    "| **Executor**       | Executes tasks in the workflow. LocalExecutor is common in single-node setups.                |\n",
    "| **DAGs Folder**    | Directory where DAG files (Python scripts) are stored.                                        |\n",
    "| **Workers**        | (Optional) Executors typically act as workers in single-node setups.                         |\n",
    "\n",
    "## One-Node Workflow Diagram\n",
    "\n",
    "```plaintext\n",
    "             +-------------------------------------+\n",
    "             |           Airflow Web UI            |\n",
    "             |    (Flask-based Visualization)      |\n",
    "             +-------------------------------------+\n",
    "                           |\n",
    "                           |\n",
    "                 +---------------------+\n",
    "                 |   Airflow Scheduler |\n",
    "                 |  (Schedules Tasks)  |\n",
    "                 +---------------------+\n",
    "                           |\n",
    "                           |\n",
    "                 +---------------------+\n",
    "                 |   Local Executor    |\n",
    "                 | (Executes Tasks)    |\n",
    "                 +---------------------+\n",
    "                           |\n",
    "                 +---------------------+\n",
    "                 | Metadata Database   |\n",
    "                 |  (SQLite by default)|\n",
    "                 +---------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Workflow of a One-Node Setup\n",
    "\n",
    "1. **DAG Creation**: DAGs define workflows and are written as Python scripts.\n",
    "2. **Scheduler**: Monitors the DAGs directory and schedules tasks based on dependencies and execution time.\n",
    "3. **Executor**: Executes tasks either sequentially or in parallel, depending on the executor type.\n",
    "4. **Web UI**: Displays DAGs, task statuses, and logs, while providing control over execution.\n",
    "5. **Metadata Database**: Tracks task states, DAG runs, and execution logs.\n",
    "\n",
    "## Common Executors in One-Node Setup\n",
    "\n",
    "- **SequentialExecutor**: Executes one task at a time, ideal for debugging and testing.\n",
    "- **LocalExecutor**: Executes multiple tasks in parallel using subprocesses on the same node.\n",
    "\n",
    "## Use Cases for One-Node Architecture\n",
    "\n",
    "- **Development and Testing**: Useful for building and debugging DAGs locally.\n",
    "- **Small-Scale Workflows**: Suitable for simple workflows with minimal resource requirements.\n",
    "\n",
    "## Summary\n",
    "\n",
    "A one-node Airflow architecture is a simple and efficient setup for local development or lightweight workflows. All components—Scheduler, Web UI, Executor, and Metadata DB—are hosted on the same machine, making it easy to manage and deploy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarification: Who Executes the Task in Airflow?\n",
    "\n",
    "In Apache Airflow, the **Executor** is responsible for **managing task execution**, but it does not always directly execute tasks. Instead, the Executor delegates tasks to other underlying mechanisms such as subprocesses or workers, depending on the Executor type.\n",
    "\n",
    "## How Task Execution Works\n",
    "\n",
    "1. **Scheduler**: Determines which tasks are ready to run based on dependencies and schedules.\n",
    "2. **Executor**: Manages and delegates the task execution to appropriate task runners.\n",
    "3. **Task Runner**: The actual execution is carried out by task runners such as subprocesses or external workers.\n",
    "\n",
    "## Key Executors and Execution Flow\n",
    "\n",
    "### 1. **SequentialExecutor**\n",
    "- Runs tasks one at a time.\n",
    "- The Scheduler executes the task directly in the same process.\n",
    "  \n",
    "### 2. **LocalExecutor**\n",
    "- Executes tasks in parallel using multiple local subprocesses.\n",
    "- **Subprocess Task Runners** handle the actual execution of tasks.\n",
    "\n",
    "### 3. **CeleryExecutor**\n",
    "- Tasks are added to a **Celery queue**.\n",
    "- **Celery Workers** consume tasks from the queue and execute them.\n",
    "- The Executor manages task distribution but does not directly execute them.\n",
    "\n",
    "### 4. **KubernetesExecutor**\n",
    "- Launches each task as a separate **Kubernetes pod**.\n",
    "- Kubernetes handles the execution of tasks in the pods.\n",
    "\n",
    "## Summary of Roles\n",
    "\n",
    "| Component      | Role                                                                                               |\n",
    "|----------------|----------------------------------------------------------------------------------------------------|\n",
    "| **Scheduler**  | Determines which tasks are ready to run and passes them to the Executor.                           |\n",
    "| **Executor**   | Delegates tasks to subprocesses or workers and manages execution flow.                             |\n",
    "| **Task Runner**| The actual entity that runs the task, e.g., a subprocess (LocalExecutor) or worker (CeleryExecutor).|\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "While the **Executor** is critical for managing and delegating tasks, it does not always execute them directly. In distributed setups like **CeleryExecutor** or **KubernetesExecutor**, the actual task execution is performed by workers or pods, not the Executor itself. This distinction is essential for understanding how Airflow scales and manages workloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in production environment we use multi node architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Execution flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
